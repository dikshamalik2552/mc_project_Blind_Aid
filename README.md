# ![ic_launcher](https://user-images.githubusercontent.com/89207778/170878029-d75dfd21-459f-4270-9966-e6864086cf3e.jpg) Blind-Aid 

Blind Aid is an android app that serves as a navigation aid for anyone who is visually challenged. 
It helps visually impaired people have smooth navigation both indoors and outdoors by detecting the objects in their path 
and providing audible directions and related information. It helps the user to know more about circumstances while moving forward 
and can also avoid dangerous situations like bumping into people, objects, or any other type of hindrance.

## Motivation behind making this app
It aims to make the visually impaired feel empowered and more independent. 
Since the ears are the second-best sensory organs for visually impaired people after the eyes, providing most of the 
information to them through audio signals would make their lives easier and more empowered. 
The Android app based solution can directly work on a smartphone and hence is a good option. 
It is reliable, portable, economical and no extra devices other than the smartphone are needed by the visually impaired.

## Salient Features 
1. Object Detection :- In this part of the module, object detection takes place. So by
this the visually imapired user will be able to identify objects in their surroundings. 
The camera of the phone opens up and objects are detected. Voice output is given to the user for the detected object.
2. Obstacle Avoidance :- This part prevents the user from bumping into obstacles and guides them via voice commands. 
This will help the visually impaired user to move safely. The camera of the phone opens up and objects are continuously detected. 
Whenever the user gets close to the obstacles the user is given a warning via speech.
3. Finding objects of daily use :- There are various objects which the users use on an everyday basis. 
So by this feature, the users are given audio commands so that they can safely reach these objects.
4. For taking input, the speech command is given and the user just has to tap on the screen making this app easy to use for the blind.

## Built With 
1. Android Studio - Used to develop Blind Aid (mobile application)
2. Java - Language used for coding Blind Aid
3. TensoflowLite - Utilized the [YOLOv4 model implementation](https://github.com/haroonshakeel/tensorflow-yolov4-tflite)(trained on the COCO dataset) for object detection
4. XML - Language used to design layouts of Blind Aid
5. Android Speech-to-Text API - Used to handle the audio interaction

## How to run the project

Step 1: Clone this GitHub Repository. <br />
Step 2: Download and install Android Studio. <br />
Step 3: Open Android Studio <br />
Step 4: Select 'open an existing Android Studio project' <br />
Step 5: Select the cloned folder <br />
Step 4: Connect your android device. <br />
Step 5: Click on run the project. <br />

## Navigating Through The App
 
Opening screen: <br />
<img src="https://user-images.githubusercontent.com/89207778/170882513-f9392080-3380-40e3-8340-2e1fefd6f2d9.jpeg" width = "200">

Object Detection: <br />
<img src="https://user-images.githubusercontent.com/89207778/170882374-aed186d0-2cdf-49de-ab2e-a9a1fa60934e.jpeg" width = "200">
<img src="https://user-images.githubusercontent.com/89207778/170882378-a73499dc-5bf5-4a5e-8ab4-52614de37959.jpeg" width = "200">
<img src="https://user-images.githubusercontent.com/89207778/170882384-2560cd06-1415-45ed-abde-698a6a75ed27.jpeg" width = "200">

Obstacle avoidance: <br />
<img src="https://user-images.githubusercontent.com/89207778/170882579-a3dfd5e1-5bf7-44ef-8a6d-939c4d1b9671.jpeg" width = "200">

Finding objects of daily use: <br />
<img src="https://user-images.githubusercontent.com/89207778/170882627-7975fd6e-d57e-492d-831d-33ffac15f20a.jpeg" width = "200">
<img src="https://user-images.githubusercontent.com/89207778/170882806-59007844-9b6e-40ad-8d8e-d5e06a76e098.jpeg" width = "200">
<img src="https://user-images.githubusercontent.com/89207778/170882615-4ce459e9-3190-4391-8e70-7ae31d171ec0.jpeg" width = "200">



